<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Esteban | Ëˆe-stÉ™-vÉ™n - /collection/energy-myths-and-realities/</title>

<meta name="description" content="Bite-sized ideas sourced from the internet">

<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="robots" content="all,follow">
<meta name="googlebot" content="index,follow,snippet,archive">
<!-- plugins -->

<link rel="stylesheet" href="/hugo-theme-console/css/terminal-0.7.1.min.css">

<link rel="stylesheet" href="/hugo-theme-console/css/animate-3.7.2.min.css">

<link rel="stylesheet" href="/hugo-theme-console/css/console.css">

<link rel="stylesheet" href="/css/hugo-easy-gallery.css">

<link rel="stylesheet" href="/css/style.css">

<script src="https://kit.fontawesome.com/0e201c44f1.js" crossorigin="anonymous"></script>
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
    
<link rel="apple-touch-icon-precomposed" sizes="57x57" href="favicon/apple-touch-icon-57x57.png" />
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="favicon/apple-touch-icon-114x114.png" />
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="favicon/apple-touch-icon-72x72.png" />
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="favicon/apple-touch-icon-144x144.png" />
<link rel="apple-touch-icon-precomposed" sizes="60x60" href="favicon/apple-touch-icon-60x60.png" />
<link rel="apple-touch-icon-precomposed" sizes="120x120" href="favicon/apple-touch-icon-120x120.png" />
<link rel="apple-touch-icon-precomposed" sizes="76x76" href="favicon/apple-touch-icon-76x76.png" />
<link rel="apple-touch-icon-precomposed" sizes="152x152" href="favicon/apple-touch-icon-152x152.png" />
<link rel="icon" type="image/png" href="favicon/favicon-196x196.png" sizes="196x196" />
<link rel="icon" type="image/png" href="favicon/favicon-96x96.png" sizes="96x96" />
<link rel="icon" type="image/png" href="favicon/favicon-32x32.png" sizes="32x32" />
<link rel="icon" type="image/png" href="favicon/favicon-16x16.png" sizes="16x16" />
<link rel="icon" type="image/png" href="favicon/favicon-128.png" sizes="128x128" />
<meta name="application-name" content="&nbsp;"/>
<meta name="msapplication-TileColor" content="#FFFFFF" />
<meta name="msapplication-TileImage" content="favicon/mstile-144x144.png" />
<meta name="msapplication-square70x70logo" content="favicon/mstile-70x70.png" />
<meta name="msapplication-square150x150logo" content="favicon/mstile-150x150.png" />
<meta name="msapplication-wide310x150logo" content="favicon/mstile-310x150.png" />
<meta name="msapplication-square310x310logo" content="favicon/mstile-310x310.png" />

<meta property="og:title" content="Energy Myths and Realities (Vaclav Smil)" />
<meta property="og:description" content="Bite-sized ideas sourced from the internet" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/collection/energy-myths-and-realities/" /><meta property="article:published_time" content="2017-03-26T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="/favicon/apple-touch-icon-57x57.png"/>

<meta name="twitter:title" content="Energy Myths and Realities (Vaclav Smil)"/>
<meta name="twitter:description" content="Hola, this is el summary"/>

    
</head>
<body class="terminal">
    <div class="container">
        <div class="terminal-nav">
          <header class="terminal-logo">
            <div class="logo terminal-prompt">
              
              
              <a href="/" class="no-style site-name">estebanvalencia.com</a>:~# <a href='/collection'>collection</a>/<a href='/collection/energy-myths-and-realities'>energy-myths-and-realities</a>/</div></header>
          <nav class="terminal-menu">
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                <li><a href="/me/" typeof="ListItem">me/</a></li>
                
                <li><a href="/collection/" typeof="ListItem">collection/</a></li>
                
                <li><a href="/photos/" typeof="ListItem">photos/</a></li>
                
            </ul>
          </nav>
        </div>
    </div>

    <div class="container animated zoomIn fast">
        
<h1>Energy Myths and Realities (Vaclav Smil)</h1><p> Mar. 26, 2017</p>

<p>Read time: 45 minutes and
    1 seconds.</p>
<!-- layouts/blog/single.html --><!-- ... -->
tags: 
 
<ul class = "list-inline">
    <li class = "pill list-inline-item">
      <a href="/tags/books/">books</a>
   </li> <li class = "pill list-inline-item">
      <a href="/tags/review/">review</a>
   </li></ul>



<p><em>This lack of interest changed with what came to be known as the first energy crisis—the increase in oil prices driven by the Organization of the Petroleum Exporting Countries (OPEC) in 1973 and 1974. This rise, from less than $2/barrel in early 1973 to more than $11/barrel by the spring of 1974 (BP 2009), was deliberately engineered by the leading oil exporters and did not take place in response to any physical shortage of the fuel. It went further than originally intended, cutting short the unprecedented period of economic expansion following World War II. It also turned the attention of individuals, organizations, and governments to the increasingly challenging task of securing a sufficient supply of sensibly priced energy. Moreover, this challenge coincided with the genesis of a new environmental consciousness and, hence, with efforts to reduce environmental pollution and prevent further ecosystemic degradation.</em></p>
<ul>
<li>Page 2 (location ~ 24-30)</li>
</ul>
<p><em>Unfortunately, some sensible policies aimed at reducing wasteful energy use were completely (and indefensibly) abandoned at this time. The best American example of this irrational retreat was the fate of the Corporate Average Fuel Economy (CAFE) regulations. Incredibly, the typical efficiency of America’s cars in the early 1970s was about the same as it had been in the early 1930s. Technical advances had brought huge efficiency gains to virtually every mode of common energy conversion, thanks to the introduction of transistors and integrated circuits, the adoption of fluorescent lights, improvements in massive two-stroke diesel engines in ships, the commercialization of jet engines and stationary gas turbines, and innovations in oil refining and in plastics and fertilizer production. But American-made cars of the early 1970s could still get only about 13 miles per gallon of gasoline, wasted at least 85 percent of the purchased fuel, and performed no better than they had before World War II—that is, deeply below the technical potential of the day (Sivak and Tsimhoni 2009).</em></p>
<ul>
<li>Page 3 (location ~ 37-45)</li>
</ul>
<p><em>By 1990, America imported 47 percent of its crude oil, compared to 37 percent in 1973. At issue here is not domestic energy self-sufficiency,3 but the enormous trade deficits created by oil imports that weaken the nation’s currency and long-term security and affect its strategic posture. In 2008, the United States bought 65 percent of its crude oil abroad, and the cost of imported oil and refined oil products was the single largest contributor—48 percent—to the country’s more than $700 billion trade deficit.4</em></p>
<ul>
<li>Page 4 (location ~ 48-52)</li>
</ul>
<p><em>Not until the early years of the new millennium, when oil prices began to rise once again, did attention return to energy supplies. During the latter half of 2003, the price of crude oil reached $25–$30/barrel, and during 2004 it came close to, and briefly even rose above, $40/barrel. The upward trend continued in 2005 and for the first eight months of 2006, and the media came to comment routinely on record high prices. In reality, no records were broken once two key price corrections—adjusting for the intervening inflation and taking into account lower oil intensity of Western economies6—were made. Until the early summer of 2008, these doubly adjusted oil prices remained well below the records set during the early 1980s. In August 2006, the weighted mean price of all traded oil peaked at more than $71/barrel; it then fell by 15 percent within a month and closed the year at about $56/barrel. But during 2007, it again rose steadily. By November it reached almost $100/barrel in trading on the New York Mercantile Exchange (NYMEX; see figure I-2a), and during the first half of 2008 that price rose by half, reaching a high of $147.27/barrel on July 11. As always, prices for the basket of OPEC oils, including mostly heavier and more sulfurous crudes, remained lower (see figure I-2b).7 But just three weeks after setting a record, oil prices fell by more than 20 percent, to about $115/barrel.</em></p>
<ul>
<li>Page 4 (location ~ 60-70)</li>
</ul>
<p><em>Ranking realistic solutions correctly in a hierarchy is important. If a global civilization is to commit trillions of dollars over the course of many decades to improve the odds of its stable existence, then it should follow the most rational, most economically rewarding, and least environmentally stressful course rather than pursuing inherently inferior alternatives. I believe that the least desirable strategy is to leave the existing excesses, inefficiencies, and irrationalities intact while spending huge sums and creating new environmental problems—some foreseeable and some not. Unfortunately, the monumental unwillingness of both institutions and individuals even to consider eliminating unnecessary conversions and the reluctance to commit vigorously to more efficiency in the remaining ones are now leading toward the embrace of inferior solutions.</em></p>
<ul>
<li>Page 6 (location ~ 83-88)</li>
</ul>
<p><em>Technical Innovation. Obviously, myths and misconceptions are found in any realm of human endeavor. Among recent notable examples, I would include a mistaken belief in an accelerated pace of technical innovation,8 the expectation of large economic gains from exploiting tropical biodiversity, and the anticipation of a stunning payoff to research in artificial intelligence. A widespread belief in the acceleration of technical advances owes a great deal to what I call Moore’s curse, the idea that the rapid and sustained improvements in the performance of microchips represent the norm in modern inventiveness.9 In reality, advances in microprocessor abilities are a highly atypical example of technical progress, as I show in chapter 8. A closer examination of tropical biodiversity was to yield a cornucopia of potent new drugs; it has not. And the quest for artificial intelligence has yielded less than astonishing results—the very logic and accomplishments of this decades-long effort are now questioned even by one of the field’s creators.10</em></p>
<ul>
<li>Page 7 (location ~ 93-101)</li>
</ul>
<p><em>Some energy myths, including the belief that energy conservation reduces overall energy consumption, are quite venerable, going back more than a century. Others—such as the claims that biofuels derived from crops, their residues (straw, stalks), or wood can displace a large share of liquid transportation fuels refined from crude oil—are more recent. Some attach themselves, barnacle-like, to any substrate. As a result, high-tech worshippers are now telling us that everything will be transformed by nanotechnology, which will, among other things, make possible electricity transmission without losses and incredibly cheap electricity generation by thin-film solar cells, or by genetic engineering, which will create new bacteria from scratch to produce hydrogen or plants to ooze biodiesel.</em></p>
<ul>
<li>Page 7 (location ~ 108-113)</li>
</ul>
<p><em>Oilmen and the government of Alberta, Canada, boast that the province’s oil sands contain more oil than has been found in Saudi Arabia, although most of it would be prohibitively expensive and environmentally ruinous to recover. The Saudis maintain that they can supply the world with enough oil for generations to come, although the state-run company has not offered any verifiable information on the country’s actual oil reserves for thirty years. Energy enthusiasts scanning more remote horizons see natural gas hydrates—frozen methane in the Arctic or deep under the sea bottom—as the ultimate fossil fuel whose enormous resources could last for centuries.</em></p>
<ul>
<li>Page 9 (location ~ 125-130)</li>
</ul>
<p><em>Renewable Energy. Much like religious sects that often preach salvation in strictly denominational terms, an army of renewable energy enthusiasts rejects other options and is certain that particular sources or conversions represent the answer to the world’s energy problems. Today’s dominant devotions are to wind, particularly in Europe, and crop-derived ethanol, now most fervently propagated in the United States, where scientists seek a fairy tale–like conversion of plant waste into liquid gold, or cellulosic ethanol. (Myths concerning biofuels and wind are dis-cussed in chapters 6 and 7, respectively.) Another large denomination trusts in photovoltaics—the direct conversion of sunlight into electricity; its adherents believe it will soon prevail everywhere, not just in sunny Arizona or Saudi Arabia. Germans were the first taxpayers forced to subsidize heavily what was at the time of its completion the world’s largest photovoltaic project—Bavaria’s Solarpark, with installed capacity of 10 MW, peak power of 6.3 MW, and an area of 250,000 m2 divided among three sites12—in one of Europe’s cloudiest locations. A facility built in a sunny location would produce better results—at least five times that rate would be generated in Sicily or Arizona, for example—but on earth, where the atmosphere interferes and nights follow days, solar conversions are always limited. A superior choice would be to put photovoltaics in the sky as fleets of satellites or, even better, on the moon, with electricity beamed back to Earth by microwaves. For many years, David Criswell, director of the Institute for Space Systems Operations at the University of Houston, has been the leading advocate of this lunar solar power.13</em></p>
<ul>
<li>Page 9 (location ~ 136-149)</li>
</ul>
<p><em>I should not forget the devotees of geothermal energy, as well as the minor renewable denominations, including those putting their faith in ocean waves, ocean currents, and ocean thermal differences. The last option involves sinking a long pipe into cold waters (&lt; 4°C, the near-constant temperature of the abyss) beneath the warm subtropical or tropical seas, whose daily high temperatures are &gt; 25°C, and using the temperature difference to generate electricity.14 Of course, there is that small problem of fundamental thermodynamics: The difference in temperature (&lt;U+0394&gt;T) between the hot and cold reservoir (a mere 20°C) is tiny compared to the difference in a large thermal electricity generating plant, where &lt;U+0394&gt;T is &gt; 500°C. Hence, the efficiency of the process is so low (typically 3–4 percent) that it may take more electricity to pump the deep cold water to the surface than is generated by the process. But most of these fervent devotions have a common goal: At the end of these renewable rainbows is a near-miraculous, clean, carbon-free hydrogen economy. And before we get there, partial energy salvations will be delivered by hybrid or electric cars (see chapter 1), by compact fluorescent lights or light-emitting diodes, by draconian carbon taxes, by massive sequestration of carbon dioxide, or by stimulation of phytoplankton growth and the subsequent burial of organic carbon in the abyss, a biosequestration method using a natural carbon pump to allow us to emit CO2 at will.</em></p>
<ul>
<li>Page 10 (location ~ 149-160)</li>
</ul>
<p><em>The myth that energy efficiency reduces energy consumption has been particularly well refuted by Rudin, Herring, and, in a book-length treatment, Polimeni and others.17 But no recent study can do better than these three sentences from Stanley Jevons’s classic treatment of the myth, written 150 years ago: It is wholly a confusion of ideas to suppose that the economical use of fuels is equivalent to a diminished consumption. The very contrary is the truth. As a rule, new modes of economy will lead to an increase of consumption according to a principle recognized in many parallel instances.18</em></p>
<ul>
<li>Page 13 (location ~ 197-202)</li>
</ul>
<p><em>Myths concerning energy have a venerable past. Galileo Galilei (1564–1642) boldly and at considerable personal peril reaffirmed the sun’s place at the center of our planetary system, but he thought that heat was a mere illusion of the senses, an outcome of mental alchemies. Francis Bacon (1561–1626), another luminary of early modern science, maintained quite inexplicably that heat could not generate motion, and vice versa. Bacon’s contemporaries believed in phlogiston—supposedly the principal component of all combustibles whose liberation (dephlogistication) during combustion leaves behind the residue of calx. This surprisingly durable myth led to many chemical cul-de-sacs before it was finally overturned. As for more recent history, it is only mildly hyperbolic to assert that current energy policy debates rest mostly on myths. These myths have ranged from a widely held belief that large oil companies have colluded in a decades-long conspiracy to block any viable alternatives to the internal combustion engine (supposedly by buying up or otherwise suppressing relevant patents) to an even more widely held conviction that higher energy efficiency lowers overall energy consumption.</em></p>
<ul>
<li>Page 14 (location ~ 214-223)</li>
</ul>
<p><em>In 1896, a Riker electric car won the first U.S. track race at Narragansett Park in Rhode Island when it decisively defeated Frank Duryea’s gasoline vehicle. On April 29, 1899, the bullet-shaped electric La Jamais Contente, driven by Camille Jenatzy, broke the 100 km/h barrier by briefly going at 105.88 km/h.3 Meanwhile, the commercial introduction of electric cars began in 1897 with a dozen Electric Carriage and Wagon Company taxicabs in New York. In 1899, U.S. carmakers produced more than 1,500 electric vehicles, compared to just 936 gasoline-powered cars.4 In 1901, Pope’s Electric Vehicle Company was both the largest manufacturer and the largest owner and operator of motor vehicles in the country.5 Other well-known makers included Anthony Electric, Baker Electric, Detroit Electric, and Studebaker. The diffusion of electric cars led to the emergence of a new infrastructure aimed at overcoming the limited range of these vehicles. By 1901, it was possible to travel by electric car from New York to Philadelphia, thanks to six charging stations that were built in New Jersey; and by 1903, Boston had thirty-six charging sites.6</em></p>
<ul>
<li>Page 17 (location ~ 259-269)</li>
</ul>
<p><em>Edison spent almost the entire first decade of the twentieth century trying to develop a high-density battery that could compete with gasoline.8 The result of this costly effort, introduced in 1909, was Edison’s nickel-iron-alkaline battery, which came to be used mainly as a dependable standby source of electricity rather than a competitive prime mover for vehicles. During the next fifteen years, improvements in gasoline engines and advances in car construction combined to make electric vehicles the losers in the vehicular evolutionary race.9 The relevant innovations included the universal adoption of assembly-line car manufacturing, introduced with Ford’s Model T in 1908; the electric starter, which eliminated cranking, patented by Charles Kettering in 1911 and introduced in Cadillacs in 1913; and Thomas Midgley’s solution to engine knock—the addition of tetraethyl lead to gasoline, beginning in 1924. Interest in electric propulsion never faded among small groups of engineering enthusiasts, but by the 1930s there were no commercial makers of electric cars.</em></p>
<ul>
<li>Page 18 (location ~ 276-284)</li>
</ul>
<p><em>In 1990, the California Energy Commission mandated that by 1998, 2 percent of all new vehicles (about 22,000 cars) sold in the state would have to be electric, and that by 2003 the share of zero-emissions vehicles—presumably mostly electrics—should reach 10 percent of the state’s car sales, or close to 150,000.10 But subsequently these requirements were greatly weakened, and none of the original goals were achieved; no truly commercial electric cars became available during the 1990s (Kirsch 2000).</em></p>
<ul>
<li>Page 20 (location ~ 293-297)</li>
</ul>
<p><em>In 2001, the California Air Resources Board redefined the goal for the year 2003: At least 10 percent of newly sold vehicles were to have low emissions, but only 2 percent were to have zero emissions.11 A year later, the now defunct DaimlerChrysler joined General Motors in a lawsuit against the California Air Resources Board asking that it repeal all zero-emissions vehicle mandates, and in 2003 GM decided to stop leasing its EV 1, a two-seat sports car powered by lead-acid batteries and produced in small numbers since 1996. This prototype was discontinued by the end of 2004 amid conspiratorial accusations that the usual suspects (large car and oil companies) wished it to fail.12 But Kirsch put it best: Electric cars have never been a replacement for America’s family sedans, nor will they now replace vans and SUVs; they are a niche product whose small market has always translated into small profits and, hence, into no more than a very reluctant embrace by major car companies.13</em></p>
<ul>
<li>Page 20 (location ~ 297-305)</li>
</ul>
<p><em>electric hybrids, now offered by all major carmakers, have not entirely eliminated the dream of the purely electric car. Enthusiasts still await its ascendancy, and media reports continue to proclaim it the next decade’s choice. In the United States this most recent wave began with the tiny Tango, the expensive Tesla Roadster, and the supposedly “game-changing” GM Volt, all of them touted as finally ushering in the era of the electric car’s triumphal ascendance. Tango was originally a Smart Car—a Mercedes-made, gasoline-powered two-seater whose city performance was rated at 4.6 L/100 km, or just over 50 mpg—converted to an all-electric drive by Hybrid Technologies (2007) in Nevada. And since we all know that people who care most about the planet live in Hollywood, nothing could make us take this vehicle more seriously than the fact that George Clooney bought it “sight unseen” and then graciously consented to be photographed while leaning languidly against its shiny minibody: “Clooney’s Tango! WoW!!!”14 The first version carried 218 kg of lithium-ion batteries and needed six to eight hours of charging; its range was advertised as 193–241 km and its top speed was 128 km/h.</em></p>
<ul>
<li>Page 21 (location ~ 312-321)</li>
</ul>
<p><em>Tesla Roadster has an even more newsworthy pedigree—media cannot stop gushing over the fact that it comes from Silicon Valley, although it really does not. Elon Musk, the founder of PayPal, set up Tesla Motors partly with his own money to produce a powerful electric car that could, according to a wide-eyed Vanity Fair writer, “spell the end of the internal-combustion engine.”17 The Tesla Motors website18 promised much more: a green future and “a peaceful solution to oil wars” through the introduction of “gasoline-free” cars.19 This is not a joke, but a quotation, and it places a very heavy burden on a frivolous machine whose retail price began at $92,000, was raised in the spring of 2007 to $98,000, and by the fall of 2008 to $109,000. In 2009 (with $7,500 of U.S. federal tax credit), it sold for $101,500. The car went into regular production in March 2008, and some 900 cars were delivered by the end of 2009. You can have a Roadster, too, if you put down an initial fee of $5,000, followed by $55,000 to lock in your production spot—and wait. My advice: Do not be surprised if the end of gasoline cars and the emergence of electric vehicle supremacy do not unfold exactly along the lines anticipated by Elon Musk and Vanity Fair. The Roadster is essentially an extended-wheelbase British Lotus Elise loaded with 6,831 lithium-ion batteries.20 The energy density of these batteries can be as high as 160 Wh/kg, which is four times that of standard lead-acid cells, and they would give the car a range of 400 km and enable it to be recharged in less than four hours. The price makes it abundantly clear that the market for this “highperformance” two-seat “sports” car is a smallish group of showoffs and I-already-have-everything-else customers. These buyers are smitten by the fact that it can reach more than 200 km/h and accelerate to 100 km/h in less than four seconds, and that it can pin a driver against the back of the seat like in a fighter plane.21 The Roadster’s pricing and appeal would have been familiar to the promoters of a similar class of cars before World War I, making it anything but a “reinvented” car.</em></p>
<ul>
<li>Page 22 (location ~ 327-344)</li>
</ul>
<p><em>The leading U.S. contender is the four-seat Chevrolet Volt, a car that is powered primarily by an electric motor (rechargeable from a standard outlet) and that uses a small (one liter) fuel engine only as a generator to extend the driving range when the battery storage gets low.24 Before its 2009 bankruptcy, GM saw the vehicle as a fundamental component of its long overdue strategy to regain its market share and reinvent itself as a competitive car company; after the emergence from its bankruptcy, GM sees Volt as a car that will reestablish its technical competence, and it hopes that the vehicle’s introduction in 2010 will put the company ahead of its competition. The production Volt, a little larger than a Honda Civic, does not look like the model unveiled in 2007, although it retains “a similar set of visual cues and some of the features that were on the concept car.”25 Chevrolet’s plan is to build 10,000 units in 2010 and 60,000 units in 2011 (priced at about $40,000 per car), and even if the post-2011 production rises by a (most unlikely) rate of 50 percent per year (compounded), there would be about 2.3 million Volts on the road by 2020, amounting to less than 1 percent of all U.S. vehicles. Volt may be a revolutionary gamble for GM, but even its best imaginable success will not transform America’s car fleet in a hurry.</em></p>
<ul>
<li>Page 24 (location ~ 355-365)</li>
</ul>
<p><em>With the global car demand forecast at more than 80 million vehicles a year by 2020, carmakers would have to boost their production of pure electrics to more than 8 million in just one decade to make Renault’s forecast a reality. How likely is that, given the fact that hybrid cars, which have been around for more than a decade, claimed less than 3 percent of the U.S. market in 2009? How readily will the requisite tens of millions of batteries be available when manufacturers are quick to unveil new, bold electric car plans but slow to commit to massive battery orders?28 And how will the car owners in large cities, where 30–60 percent of all cars are parked curbside, charge their vehicles?</em></p>
<ul>
<li>Page 25 (location ~ 372-377)</li>
</ul>
<p><em>With nearly 55 million vehicles total on German roads in 2010, electrics would claim less than 2 percent of all German passenger cars by 2020. Any beliefs in an imminent massive takeover of the global car market by pure electrics are thus highly unrealistic. But even if electrics were to do better than can be realistically expected, we still have to look at what flipping the switch would do to actual energy demand. Only simple algebra and a string of realistic assumptions, based on the typical performance of electric cars and on the latest transportation statistics, are needed to calculate what this would mean in America’s case. In calculating the overall burden that an entirely or partly electric fleet would put on the country’s electricity supply, we would be naïve to assume that either converted Smarts or PT Cruisers, and even less so $100,000 Roadsters, will be America’s choice for daily transportation. An electric version of a car whose size would correspond to today’s typical American vehicle (a composite of passenger cars, SUVs, vans, and light trucks) would require at least 150 Wh/km; and the distance of 20,000 km driven annually by an average vehicle would translate to 3 MWh of electricity consumption.</em></p>
<ul>
<li>Page 25 (location ~ 383-391)</li>
</ul>
<p><em>But Kintner-Meyer, Schneider, and Pratt (2007) would say that these calculations of additional electricity generating capacity are largely incorrect, because as of 2001 (the baseline of their calculations), the United States could produce enough electricity from its underutilized capacity to power about 73 percent of all light-duty vehicles on the road at that time, that is about 173 million cars, pickups, and SUVs. Their calculation assumes plug-in hybrid electric vehicles with batteries able to satisfy average drives of 33 miles (or about 53 km) a day, with all the additional generation coming only from coal- and natural gas–fired power plants. It would call for increasing the average capacity factors of those plants from, respectively, about 73 percent and 40 percent to as much as 85 percent, and for recharging the cars with electricity produced in excess of the existing average load at all hours; if the charging periods were only between 6 p.m. and 6 a.m., the additional electricity generated without adding any new capacity would power not 73 percent, but only about 43 percent, of all light-duty vehicles.35</em></p>
<ul>
<li>Page 27 (location ~ 403-410)</li>
</ul>
<p><em>It was clearly a generic futuristic vision, and Strauss did not even make an explicit link between nuclear fission and electricity too cheap to meter—a fact that led some to argue later that he had in mind the commercialization of nuclear fusion. (Commercial fission uses neutrons to split nuclei of the heaviest natural element, uranium; fusion, the joining of the lightest nuclei, powers stars and hydrogen bombs, and its commercialization remains as elusive as ever.) And his statement should obviously not be taken literally, as even “free” electricity would have to be transformed, transmitted, and distributed to users, requiring the construction and maintenance of an extensive and costly infrastructure. Perhaps the most logical explanation of the statement is that he wished to suggest electricity would be so cheap that households could be charged a fixed monthly or annual rate rather than pay for the amount actually consumed. But all these qualifications are irrelevant. Strauss’s phrase acquired a life of its own, as it came to embody technical hubris—the unrealistically boastful attitude of arrogant innovators—and as it has been used by the critics of nuclear power to disparage the industry’s credibility ever since. The reality that surrounded that unfortunately hyperbolic statement was, however, more complex. Questioning the technique’s maturity, costs, and potential risks, many power engineers and utility economists were not at all enthusiastic about the push to develop nuclear generation and raised doubts about whether it was even needed in a modern electricity supply. This group included the first chairman of the U.S. Atomic Energy Commission, David E. Lilienthal. In 1955, during the first International Conference on the Peaceful Use of Atomic Energy, Lilienthal wrote in his private journal that the recent history of nuclear development “is characterized more by salesmanship, propaganda, and overzealousness than sense. These men are fanatics or zealots; caveat zealot!”2 But these damning words, unlike Strauss’s vision, have not become part of nuclear lore.</em></p>
<ul>
<li>Page 32 (location ~ 489-504)</li>
</ul>
<p><em>An important consideration for the United States was certainly eagerness to ease the sense of guilt over the bombing of Hiroshima and Nagasaki by demonstrating a peaceful use of fission. The political and strategic considerations—the desire not to be bested by the Soviet Union or, for that matter, by the United Kingdom and Canada, two Western countries that formulated their own early nuclear programs—were no less important. In December 1953, after the explosion of the first Soviet thermonuclear device, President Dwight D. Eisenhower announced his Atoms for Peace plan. The plan sought to demonstrate America’s nonthreatening, peaceful nuclear capability and was also designed to attract the attention of nonaligned countries interested in new forms of energy—the economical generation of electricity for domestic use was not its primary goal. The plan required an operational reactor, and the only reactor design available at that time for uses other than weapons production was the one used to power new U.S. submarines. The first nuclear-powered submarine, Nautilus, was launched in January 1954 following rapid innovation efforts led by Admiral Hyman G. Rickover.4</em></p>
<ul>
<li>Page 34 (location ~ 515-523)</li>
</ul>
<p><em>In 1971, Glenn Seaborg, chairman of the U.S. Atomic Energy Commission and a Nobel Prize–winning chemist, delivered an address at the fourth International Conference on the Peaceful Uses of Atomic Energy that was even more visionary than Strauss’s 1954 talk.7 By the year 2000, Seaborg said, nuclear energy’s “unimagined benefits” were to improve the quality of life for most of the world’s population. Fission reactors would not only generate nearly all of the world’s electricity; they would also transform the world’s agriculture by energizing food production complexes. Here Seaborg was promoting the large nuclear complexes (“nuplexes”) first proposed by Richard L. Meier in 1956 and later elaborated by the Oak Ridge National Laboratory.8 These complexes, centered on large nuclear plants and located in coastal desert areas, were to produce energy for the desalinization of sea water, synthesis of fertilizers, industrial activities, and intensive crop cultivation that would make deserts habitable. Many other nuclear wonders were to be in place by the year 2000: “Giant earth-stationary satellites bearing compact nuclear reactors will broadcast television programs”; nuclear-powered tankers and other merchant ships “will almost certainly ply the seas”; “peaceful nuclear explosives will be employed on a widespread scale” in underground mineral mining and used to modify the earth’s surface, alter river flows, and construct new canals and new harbors in Alaska and Siberia; and “nuclear propulsion” would carry men to Mars.9</em></p>
<ul>
<li>Page 35 (location ~ 533-544)</li>
</ul>
<p><em>In reality, the quintupling of world oil prices had a decidedly negative effect on nuclear fortunes. Higher oil prices, higher inflation, lower rates of economic growth, and a belated effort to conserve electricity were key factors that helped reverse the decades-long era of high annual growth in electricity demand. Until 1970, this demand had doubled roughly every ten years, with average annual growth of around 7 percent; but after 1973 the growth rates dropped to just 2–3 percent per year, and in some nations and regions they entered a prolonged period of stagnation. The United States went from capacity shortages during the early 1970s to a large capacity surplus during the 1980s.</em></p>
<ul>
<li>Page 37 (location ~ 564-569)</li>
</ul>
<p><em>As a result, unit costs began to escalate (see figure 2-1). A plant whose construction began in 1980 for completion in 1992 was expected to cost well over $3,000/kWe, whereas a unit completed in 1975 after less than six years of construction cost just $240/kWe. And worse was to come: Diablo Canyon plant in California, whose original projected cost was $450 million, cost $4.4 billion, while New York’s Shoreham, nine years behind schedule, cost $6 billion rather than the projected $241 million. By the late 1980s, the cost overruns became so bad that a detailed study of nuclear plants under construction concluded that, with the possible exception of those in the Southeast, the least expensive choice was not to complete them—and many were not completed.15</em></p>
<ul>
<li>Page 38 (location ~ 576-582)</li>
</ul>
<p><em>In contrast, although the initially rushed adoption of conventional reactors stopped far short of early expectations of their market penetration, hundreds of them now work reliably in more than thirty countries, and nuclear fission has made a real difference globally, especially in some nations. In 2008, the world had 439 nuclear power plants with a total net installed capacity of about 371 GW—only about 11 percent of the global total. But nuclear reactors have load factors—that is, the percentage of time they are actually used to produce electricity—significantly higher than those of units powered by fossil fuels or water. Well-run nuclear power plants can operate 95 percent of the time, and the U.S. average is now nearly 92 percent, significantly up from about 75 percent in 1995.24 This compares to typical rates of 65–75 percent for coal-fired stations, 40–60 percent for hydrogenation, and 25 percent for wind turbines. Because of this higher load factor, nuclear fission now generates about 16 percent of the world’s electricity.25 For some individual nations, the share is much higher—in France it is 78 percent, in Japan about 30 percent, and in the United States nearly 20 percent.26</em></p>
<ul>
<li>Page 41 (location ~ 618-627)</li>
</ul>
<p><em>Long-lasting European opposition to nuclear generation was hugely reinforced by an incomparably worse accidental core meltdown and the release of radioactivity amounting to about 5 percent of the reactor’s radioactive core during the Chernobyl disaster in Ukraine in May 1986.36 The fact that this accident arose from the combination of a flawed reactor design with no proper containment structure, unacceptable operating procedures, and inadequately trained personnel (a combination that did not obtain at any Western plant) became irrelevant as a radioactive plume drifted across the continent and contaminated large areas of Eastern and Northern Europe. Nor has it mattered that actual health consequences were far less tragic than predicted.37 The contrast between the repeated assurances of safety and the images of evacuees, sick children, and abandoned swaths of Ukraine was too powerful, and Chernobyl will continue to cast a long shadow over nuclear power for decades to come, particularly in Europe. Any serious student of comparative risk must feel exasperated that the years of analyses and operating experience proving Western nuclear power a highly acceptable choice apparently count for nothing. After 9/11, the fear of nuclear terrorism was added to perennial worries about the permanent storage of highly radioactive wastes, the real cost of nuclear generation, the chance of catastrophic reactor failures, near- and long-term environmental impacts, and the link between electricity from fission and nuclear proliferation.38</em></p>
<ul>
<li>Page 43 (location ~ 647-658)</li>
</ul>
<p><em>Perhaps the most radical solution is for the reactors to be buried underground and to operate without any refueling, as was envisaged by Edward Teller in one of his last contributions to the field of nuclear physics.41 All this means that an early and substantial nuclear comeback is unlikely either in North America or in Europe. The only countries that continue to add significant nuclear capacity and plan further expansion are France (which based its successful nuclear program on a modular deployment of a Westinghouse PWR design), Japan, South Korea, India, Russia, and China. China has particularly bold plans, officially set at 40 GW of capacity by the year 2020 but recently estimated to reach perhaps as much as 70 GW in that year.42 By comparison, the U.S. nuclear capacity is now just over 100 GW.43 But much more is required if the world’s nuclear industry is even to maintain its current share of electricity generation until 2030.</em></p>
<ul>
<li>Page 44 (location ~ 664-671)</li>
</ul>
<p><em>have elsewhere called the use of nuclear fission for electricity generation a successful failure.45 No other mode of primary electricity production was commercialized as rapidly as the first generation of fission reactors; only about twenty-five years elapsed between the first sustained chain reaction that took place at the University of Chicago on December 2, 1942, and the flood of new plant orders after 1965. But no other mode of electricity production has fallen so far short of its initial expectations.46 And no other mode of energy production has received such generous public subsidies. U.S. data show that the nuclear industry was the recipient of no less than 96 percent of all funds, amounting to about $145 billion in 1998 dollars, that were appropriated by the U.S. Congress for energy-related research and development between 1947 and 1998.47</em></p>
<ul>
<li>Page 44 (location ~ 674-680)</li>
</ul>
<p><em>Nuclear electricity is now as important globally as hydroelectricity, and even relatively modest but steady capacity additions should keep that share, now close to 20 percent, from falling during the next ten to twenty years. But the economics of nuclear generation have always been in dispute, given the many externalities that have not been properly accounted for. They range from long-term health effects seen among uranium miners to the cost of decommissioning the shut-down reactors. Foremost among these concerns is the fact that no country, not even France with its bold commitment to nuclear generation, has thus far devised an acceptable method for permanently storing a relatively small volume of highly radioactive waste that must be sequestered for thousands of years.</em></p>
<ul>
<li>Page 45 (location ~ 681-686)</li>
</ul>
<p><em>This failure to date, of course, is not proof that effective solutions are impossible. It demonstrates only the enormous influence that a mistaken public risk perception can have on government policy, and also suggests the consistently inept bureaucratic handling of the challenge so far. This discouraging experience is even more incomprehensible given the fact that nuclear generation is the only low-carbon-footprint option that is readily available on a gigawatt-level scale. That is why nuclear power should be part of any serious attempt to reduce the rate of global warming; at the same time, it would be naïve to think that it could be (as some suggest) the single most effective component of this challenge during the next ten to thirty years. The best hope is for it to offer a modest contribution.</em></p>
<ul>
<li>Page 45 (location ~ 686-692)</li>
</ul>
<p><em>For Lovins, the value of the soft-energy path reached far beyond its powerful yet benign solutions to the world’s energy problems. Harnessing these energies would be not only economical but also “elegant,” a quality cherished by engineers; and small-scale, decentralized conversions would particularly benefit the poor, since they would contribute “promptly and dramatically to world equity and order.” Indeed, such conversions would foster the diffusion of democracy “from the ground up,” even as they spread the virtues of community resilience and self-sufficiency and provided safe and “ecologically inoffensive” alternatives to an inherently destructive and risky hard-energy path.5 Although the paper had thirty-six citations, it did not refer, most curiously, to what was clearly one of its major inspirations. Ernst F. Schumacher’s Small Is Beautiful was a slim volume by a British economist, statistician, and long-time adviser to the National Coal Board that rapidly established its author as the most influential advocate of smallness.6 His approach to economic development was based on four fundamental dicta: Make things small where possible, reduce capital intensity, make the process simple, and make it nonviolent. Schumacher’s work was a key theoretical justification for a new form of a globally applicable economic development strategy whose critical ingredient was what came to be known as intermediate or appropriate technology. The invariably small-scale and simple techniques and methods associated with this technology stood in obvious contrast to large-scale, high-tech approaches, which were seen as grossly mismatched with the enormous needs (whether for jobs, food, or energy) of poor, populous countries. Without any doubt, Lovins’s Foreign Affairs article should be seen as a specific application to energy affairs of the general Schumacherian principle.</em></p>
<ul>
<li>Page 47 (location ~ 716-730)</li>
</ul>
<p><em>The only key point that Lovins got right in his essay is that the version of the hard path he portrayed did not become a reality. Of course, forecasting this was no great achievement; it was merely a matter of outlining an extreme prospect (that is, the most extravagant version of the myth of nuclear power) to argue that it should not, and almost certainly would not, happen.</em></p>
<ul>
<li>Page 48 (location ~ 734-737)</li>
</ul>
<p><em>Lovins’s career started during the early 1970s with an aggressive opposition to the nuclear industry, a vigorous dismantling of exaggerated expectations for its growth, and a detailed critique of the safety, environmental, and political implications of relying primarily on nuclear fission. This back-ground enabled Lovins to expose the extreme hard-path myth, including a simplistic long-term energy forecast of unchecked continuation of previous exponential growth that implied the existence of as many as 800 nuclear reactors by the year 2000, when the United States would consume more than 150 EJ of primary energy.</em></p>
<ul>
<li>Page 49 (location ~ 737-741)</li>
</ul>
<p><em>Renewable energies other than large-scale hydroelectric power (an energy conversion that, according to Lovins’s own criteria, obviously does not belong to the soft path) provided just over 4 percent of all U.S. primary energy consumption. In fact, more than 90 percent of this total was accounted for by the burning of logging residues in wood-processing and pulp and paper enterprises, by ethanol production in large-scale industrial facilities, and by generation of electricity in large commercial wind farms. In other words, small-scale, decentralized energy conversions contributed less than 0.5 percent of the U.S. primary energy supply in 2000, rather than the 33 percent envisaged by Lovins in 1976, or less than 0.5 EJ rather than 33 MJ. Missing a target by 98.5 percent—that is, getting it wrong over the course of twenty-four years by a factor of more than sixty—does not constitute brilliant foresight. The indisputable disappearance of the original soft-energy mirage is the most telling illustration of the vastly unrealistic hopes that were invested in the less-trodden energy future during the late 1970s.</em></p>
<ul>
<li>Page 50 (location ~ 757-765)</li>
</ul>
<p><em>Lovins was not the only enthusiastic promoter of a soft, decentralized energy nirvana. A massive report of nearly 1,800 pages by the InterTechnology Corporation concluded that solar energy could supply 36 percent of America’s industrial process heat by 2000, including at least 70 percent of all heat for applications requiring temperatures less than about 300°C.13 A Harvard Business School study suggested that by the year 2000 the United States could “reasonably” satisfy 20 percent of its total energy needs through solar sources, with solar heating, both active and passive, being the single largest contributor; no new conversion techniques would be needed to achieve that goal.14 In the same year, Hayes was a bit more conservative, envisaging about 25 percent of all U.S. energy coming from decentralized renewable resources within the next fifty years.15</em></p>
<ul>
<li>Page 51 (location ~ 776-782)</li>
</ul>
<p><em>Mao’s thought in action preceded Schumacherian preaching by fifteen years: The Great Leap Forward, launched in 1958, was based on a delusionary idea that a poor, underdeveloped nation could catch up with the world’s most advanced economies in a single, frenzied spurt of a few years. This impossible goal involved mass replication of primitive, small-scale techniques, with hundreds of millions of people forced to cut down trees, mine poor iron ore and coal, and build primitive backyard furnaces to smelt substandard iron. But this leap ended in the worst man-made famine in history, in which more than 30 million people died, and in a reversion to normal economic practices.19 The excesses of the late 1950s were not repeated during the 1970s, but smallness and simplicity once again influenced energy policy in China. Small coal mines, small hydrostations, and family-sized biogas digesters were constructed and operated during this period. The latter two were perfect embodiments of renewable energy softness, and, as such, they garnered plenty of uncritical admiration by the Western devotees of smallness. The practice of small-scale biogas generation was based on eclectic inputs available even in poor villages. Animal dung, human feces, pieces of vegetation (crop stalks, straw, grass clippings, leaves), garbage, and wastewater were sealed up in insulated brick or concrete containers (digesters) and left to decompose. Biogas produced by anaerobic methanogenic bacteria is 55–70 percent CH4 (methane) and 30–45 percent CO2, and its energy content is 22–26 MJ/m3. Villagers used it for cooking and lighting, and a typical 10 m3 digester was claimed to cover these needs for a typical south Chinese family of five; promotion of the practice began in the early 1970s in Sichuan, where more than 30,000 digesters were built by the end of 1973 and more than 400,000 of them were reported to be in operation by the middle of 1975.20 In 1978, China’s official goal was to have 20 million digesters in 1980 and 70 million units by 1985. But once their diffusion ceased to be a subject of Maoist campaigns—and once the peasants, freed by Deng’s reforms to manage their activities for profit, saw them from a purely economic perspective—the bubble burst. As rural China sought to improve its standard of living, and as villagers began to engage in various private activities, the digester total fell below 4 million by 1984. Although the total later rose a bit because many farmers found the use of larger digesters for animal waste control profitable, it never surpassed the 1979 peak. Moreover, most of the remaining biogas digesters were unable to produce enough fuel to cook rice three times a day, still less every day for four seasons.21</em></p>
<ul>
<li>Page 52 (location ~ 792-813)</li>
</ul>
<p><em>Small-scale hydrostations made more sense. Thanks to its mountainous terrain, no other country has a higher hydrogenerating potential than China, and by building small stations the Chinese were only following many historical precedents, as such installations were common during the early stages of electrification in North America, Europe, and Japan. China’s small hydrostation program began as part of a massive water conservancy effort during the Great Leap years, and Maoist planners had visions of no less than 2.5 GW of aggregate capacity in 1967. In reality, when the Great Leap collapsed in famine, the total amounted to less than 500 MW.23</em></p>
<ul>
<li>Page 54 (location ~ 822-826)</li>
</ul>
<p><em>Less than a decade after the end of Maoism, the Chinese pendulum had swung dramatically the other way, and that trend has only intensified in recent years. Gone are the campaigns promoting tiny biogas digesters and small hydrostations of less than 50 kW. The country that was once seen as the greatest potential beneficiary of small-scale soft-energy conversions and the most convincing embodiment of the Schumacherian future has become a serial builder of energy megaprojects. In 2006, China commissioned the equivalent of ninety large (1 GW) coal-fired electricity generating plants, or nearly as much as the entire French capacity. It also completed the world’s largest hydrostation, Sanxia on the Yangzi, with 18.2 GW—about 45 percent larger than the second-largest project, Itaipú on the Paraná between Brazil and Paraguay. By any criteria, the hard-energy path has fared only too well in China.</em></p>
<ul>
<li>Page 55 (location ~ 836-842)</li>
</ul>
<p><em>Lovins’s statement that renewable energy flows are “matched in scale and in geographic distribution to end-use needs” is similarly misleading, as the Chinese found after wasting so much effort on an ideologically enforced soft-and-small approach. More than half of humanity is now living in cities, and an increasing share inhabits megacities from São Paulo to Bangkok, from Cairo to Chongqing, and megalopolises, or conglomerates of megacities.25 How can these combinations of high population, transportation, and industrial density be powered by small-scale, decentralized, soft-energy conversions? How can the fuel for vehicles moving along eight- or twelve-lane highways be derived from crops grown locally? How can the renters of smallish cubicles on the thirtieth floor of high-rises—facing even taller walls just a few meters away—extend their individual solar heaters or wind turbines from their windows? How can the massive factories producing microchips or electronic gadgets for the entire planet be energized by attached biogas digesters or by tree-derived methanol? And while some small-scale renewable conversions can be truly helpful to a poor rural household or to a small village, they cannot support such basic, modern, energy-efficient industries as iron and steel making, nitrogen fertilizer synthesis by the Haber-Bosch process, and cement production. In 1978, Lovins claimed that “soft technologies are . . . increasingly used in practice, to construct smooth transitions (over 50 years or so) to virtually complete reliance on appropriate renewable energy sources.”26 As I have quantified in detail, three decades into this transition (excluding large-scale hydrogeneration, which has been a well-established, centralized, hard technique for decades), the United States derives less than 1 percent of its primary energy from new renewables and less than 0.1 percent from smaller, decentralized conversions. Centralized electricity generation still dominates, and there are no signs of its imminent retreat. And all those relatively small contributions by renewables are based on larger, and increasingly numerous, commercial installations. The current biofuels craze relies on the large-scale industrial conversion of cane and corn, not on any household-size units. Wind installations are now packing ever larger individual turbines into projects rated at hundreds of megawatts, onshore and offshore.</em></p>
<ul>
<li>Page 56 (location ~ 856-873)</li>
</ul>
<p><em>The fundamental problem with the notion of predicting a peak for oil extraction is that it rests on three simple assumptions—that recoverable oil resources are known with a high level of confidence, that they are fixed, and that their recovery is subsumed by a symmetrical production curve—which happen not to be true. These three claims mix incontestable facts and sensible arguments with indefensible assumptions, and they caricature complex processes and ignore those realities that do not fit preconceived conclusions. There is, obviously, a finite amount of liquid oil in the earth’s crust, but our estimates of this grand total remain uncertain.</em></p>
<ul>
<li>Page 69 (location ~ 1050-1055)</li>
</ul>
<p><em>Moreover, a much documented reality is that an oilfield’s ultimate recovery tends to grow with time because of additional drilling and higher recovery rates; EUR oil for recently discovered fields thus definitely underestimates their eventual cumulative production. Nehring demonstrated how this reality invalidates predictions based on Hubbert’s method for two of the leading oil-producing regions in California and in Texas and New Mexico.14</em></p>
<ul>
<li>Page 70 (location ~ 1059-1062)</li>
</ul>
<p><em>Let us counter the claims of radical peak-oilers (those who see the peaking oil output as the beginning of civilization’s dramatic collapse) calmly. Extraction of any mineral resource must eventually decline and cease, whether due to actual physical exhaustion of a particular deposit or, much more commonly, for economic reasons, as the rising financial cost and falling net energy return force the use of alternatives. Obviously, conven-tional crude oil will not be an exception. It is fairly probable that its annual global extraction will peak within the next two decades, and it is inevitable that its share of the world’s primary energy supply will continue to decline. In 1980 oil provided 44 percent of the global primary energy supply, by 2000 it was down to 41 percent, and in 2009 it stood at less than 35 percent (though its absolute extraction in 2008 was nearly 32 percent above the 1980 level).30</em></p>
<ul>
<li>Page 79 (location ~ 1210-1217)</li>
</ul>
<p><em>Paradoxically, our very wastefulness (itself a function of decades of inexpensive supply) is a major factor working in our favor. As I have argued,31 and as the 2000-Watt Society project of the Swiss Federal Institute of Technology (ETH) tries to demonstrate,32 a decent quality of life in the world’s affluent countries could be secured even if they were eventually to halve today’s energy demand. Unless we believe, preposterously, that human inventiveness and adaptability will cease the year the world reaches the peak annual output of conventional crude oil, we should see that milestone—whenever it comes—as a challenging opportunity rather than as a reason for cult-like thinking and paralyzing anxiety.</em></p>
<ul>
<li>Page 81 (location ~ 1229-1234)</li>
</ul>
<p><em>No common underlying process explains the gradual nature of energy transitions. In the case of primary energy supply, the time span needed for significant market penetration is mostly a function of financing, developing, and perfecting necessarily massive and expensive infrastructures. For example, the world oil industry handles about 30 billion barrels annually, or 4 billion tons, of liquids and gases. It extracts the fuel in more than a hundred countries, and its facilities range from self-propelled geophysical exploration rigs to sprawling refineries and include about 3,000 large tankers and more than 300,000 miles of pipelines.18 Even if an immediate alternative were available, writing off this colossal infrastructure that took more than a century to build would amount to discarding an investment worth well over $5 trillion—and it is quite obvious that its energy output could not be replicated by any alternative in a decade or two. In the case of prime movers, there is often inertial reliance on a machine that may be less efficient (steam engine, gasoline-fueled engine) than a newer machine but whose marketing and servicing are well established and whose performance quirks and weaknesses are well known; the concern is that rapid adoption of a superior converter may bring unexpected problems and setbacks. Predictability may, for a long time, outweigh a potentially superior performance, and the diffusion of new converters may be slowed down by complications associated with new machines. One such complication pertains to the high particulate emissions of early diesels; another arises from new supply-chain requirements—for example, sufficient refinery capacity to produce low-sulfur diesel fuel, or the availability of filling stations dispensing alternative liquids.</em></p>
<ul>
<li>Page 151 (location ~ 2312-2324)</li>
</ul>



        <div class="footer">
   <div class = 'row' style='text-align:center;'><a href = "/privacy">Terms and Privacy</a>&nbsp;-&nbsp;<a href = "/tags">Index</a></div>
   <div class = 'row' style='text-align:center;'><em>â€œThe difference between a flower and a weed is judgement.â€</em> â€“ Unknown</div>
   
   </br>
   
   
</div>
    </div>
  </body>
</html>
